# üöÄ Guide de Benchmark LLM pour MacBook Pro M1

Ce guide vous aide √† benchmarker diff√©rents mod√®les LLM (7-8B) sur votre MacBook Pro M1 16GB.

## üìã Pr√©paration

### 1. Installation d'Ollama (Recommand√© - Le plus simple)

```bash
# Installer Ollama
brew install ollama

# D√©marrer le service Ollama
ollama serve

# T√©l√©charger les mod√®les (dans un autre terminal)
ollama pull llama3.1:8b
ollama pull qwen2.5:7b
ollama pull mistral:7b
ollama pull llama3.2:3b
```

### 2. Configuration MLX (Optionnel - Meilleure performance)

MLX est optimis√© pour Apple Silicon et offre g√©n√©ralement les meilleures performances sur M1/M2.

**IMPORTANT:** MLX ne peut ex√©cuter qu'un seul mod√®le √† la fois!

```bash
# Installer mlx-lm
pip install mlx-lm

# D√©marrer le serveur MLX avec UN mod√®le
mlx_lm.server --model mlx-community/Meta-Llama-3-8B-Instruct-4bit

# Dans benchmark.py, d√©commentez et configurez (ligne 89-90):
# MLX_CURRENT_MODEL = "mlx-community/Meta-Llama-3-8B-Instruct-4bit"
```

**Pour tester plusieurs mod√®les MLX:**

1. **Tester le premier mod√®le:**
   ```bash
   # Terminal 1: D√©marrer serveur
   mlx_lm.server --model mlx-community/Meta-Llama-3-8B-Instruct-4bit

   # Terminal 2: √âditer benchmark.py
   # MLX_CURRENT_MODEL = "mlx-community/Meta-Llama-3-8B-Instruct-4bit"

   # Lancer benchmark
   uv run src/agno/mlx/benchmark.py
   ```

2. **Tester le deuxi√®me mod√®le:**
   ```bash
   # Terminal 1: Ctrl+C puis red√©marrer avec nouveau mod√®le
   mlx_lm.server --model mlx-community/Qwen2.5-7B-Instruct-4bit

   # Terminal 2: √âditer benchmark.py
   # MLX_CURRENT_MODEL = "mlx-community/Qwen2.5-7B-Instruct-4bit"

   # Relancer benchmark
   uv run src/agno/mlx/benchmark.py
   ```

**Mod√®les MLX recommand√©s:**
- `mlx-community/Meta-Llama-3-8B-Instruct-4bit` - Excellent √©quilibre
- `mlx-community/Qwen2.5-7B-Instruct-4bit` - Tr√®s bon en multilingue
- `mlx-community/Mistral-7B-Instruct-v0.3-4bit` - Bon raisonnement

### 3. Configuration HuggingFace (Optionnel - N√©cessite internet)

```bash
# 1. Cr√©er un compte sur https://huggingface.co
# 2. G√©n√©rer un token: https://huggingface.co/settings/tokens
# 3. Ajouter le token dans votre fichier .env
echo "HF_TOKEN=hf_your_token_here" >> .env

# 4. Accepter les licences des mod√®les (si n√©cessaire)
# Visitez et cliquez "Agree and access repository":
# - https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
# - https://huggingface.co/Qwen/Qwen2.5-7B-Instruct
# - https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
```

## üèÉ Ex√©cution du Benchmark

### Sc√©nario 1: Ollama + HuggingFace uniquement (simple)

```bash
# Premi√®re ex√©cution (t√©l√©chargement)
uv run src/agno/mlx/benchmark.py

# Deuxi√®me ex√©cution (benchmark r√©el)
uv run src/agno/mlx/benchmark.py

# MLX sera automatiquement skipp√© (MLX_CURRENT_MODEL = None)
```

### Sc√©nario 2: Ollama + HuggingFace + UN mod√®le MLX

```bash
# Terminal 1: D√©marrer serveur MLX
mlx_lm.server --model mlx-community/Meta-Llama-3-8B-Instruct-4bit

# Terminal 2: √âditer benchmark.py (ligne 89)
# MLX_CURRENT_MODEL = "mlx-community/Meta-Llama-3-8B-Instruct-4bit"

# Lancer benchmark
uv run src/agno/mlx/benchmark.py
```

### Sc√©nario 3: Benchmark complet avec TOUS les mod√®les MLX

Pour tester les 3 mod√®les MLX, vous devez faire **3 ex√©cutions s√©par√©es**:

**Ex√©cution 1 - Llama 3:**
```bash
# Terminal 1
mlx_lm.server --model mlx-community/Meta-Llama-3-8B-Instruct-4bit

# Terminal 2: √âditer benchmark.py
# MLX_CURRENT_MODEL = "mlx-community/Meta-Llama-3-8B-Instruct-4bit"
uv run src/agno/mlx/benchmark.py
# Sauvegarder les r√©sultats quelque part
```

**Ex√©cution 2 - Qwen 2.5:**
```bash
# Terminal 1: Ctrl+C puis
mlx_lm.server --model mlx-community/Qwen2.5-7B-Instruct-4bit

# Terminal 2: √âditer benchmark.py
# MLX_CURRENT_MODEL = "mlx-community/Qwen2.5-7B-Instruct-4bit"
uv run src/agno/mlx/benchmark.py
# Sauvegarder les r√©sultats
```

**Ex√©cution 3 - Mistral:**
```bash
# Terminal 1: Ctrl+C puis
mlx_lm.server --model mlx-community/Mistral-7B-Instruct-v0.3-4bit

# Terminal 2: √âditer benchmark.py
# MLX_CURRENT_MODEL = "mlx-community/Mistral-7B-Instruct-v0.3-4bit"
uv run src/agno/mlx/benchmark.py
# Sauvegarder les r√©sultats
```

**Note:** Vous pouvez comparer manuellement les r√©sultats des 3 ex√©cutions pour choisir le meilleur mod√®le MLX.

## üìä Types de tests effectu√©s

Le benchmark teste 4 types de t√¢ches:

1. **Q&A Simple** - Questions-r√©ponses de base
2. **Code Generation** - G√©n√©ration de code Python
3. **Reasoning** - R√©solution de probl√®mes logiques
4. **Summary** - R√©sum√© de texte

## üéØ Que tester?

### Configuration minimale (Ollama uniquement)
- Temps: ~5-10 minutes
- Mod√®les: 4 mod√®les Ollama
- Tests: 16 tests (4 mod√®les √ó 4 t√¢ches)

### Configuration compl√®te (Tous les providers)
- Temps: ~30-60 minutes (ou plus avec MLX)
- Mod√®les: 10 mod√®les (4 Ollama + 3 MLX + 3 HuggingFace)
- Tests: 40 tests
- **Note MLX:** N√©cessite 3 ex√©cutions s√©par√©es du script
  - Une ex√©cution par mod√®le MLX (red√©marrer le serveur entre chaque)
  - Ou testez seulement un mod√®le MLX (configurez MLX_CURRENT_MODEL)

## üìà R√©sultats attendus sur M1 16GB

### Vitesse (tokens/seconde)
- **MLX (4-bit)**: ~40-60 tok/s (le plus rapide)
- **Ollama**: ~20-40 tok/s
- **HuggingFace API**: ~10-20 tok/s (variable, d√©pend de la connexion)

### Qualit√© (subjective)
- **Llama 3.1 8B**: Excellente qualit√© g√©n√©rale
- **Qwen 2.5 7B**: Meilleur en multilingue (fran√ßais)
- **Mistral 7B**: Bon pour le raisonnement

### Usage m√©moire
- Mod√®les 3B: ~4-6 GB RAM
- Mod√®les 7-8B: ~8-12 GB RAM
- 4-bit quantization (MLX): ~4-6 GB pour 8B

## üîß Personnalisation

### Modifier les mod√®les test√©s

√âditez `benchmark.py` (lignes 67-87):

```python
# Ajouter/supprimer des mod√®les Ollama
OLLAMA_MODELS = [
    "llama3.1:8b",
    # Ajoutez vos mod√®les ici
]

# Ajouter/supprimer des mod√®les MLX
MLX_MODELS = [
    "mlx-community/Qwen2.5-7B-Instruct-4bit",
    # Ajoutez vos mod√®les ici
]
```

### Modifier les t√¢ches de test

√âditez `benchmark.py` (lignes 38-59):

```python
TEST_CASES = [
    {
        "name": "Ma t√¢che custom",
        "prompt": "Votre prompt ici",
        "category": "custom"
    },
    # Ajoutez vos tests ici
]
```

## ‚ùì Troubleshooting

### Ollama: "model not found"
```bash
ollama list  # V√©rifier les mod√®les install√©s
ollama pull <model-name>  # T√©l√©charger un mod√®le manquant
```

### MLX: "Connection refused"
```bash
# Le serveur MLX n'est pas d√©marr√©
mlx_lm.server --model mlx-community/Meta-Llama-3-8B-Instruct-4bit
```

### HuggingFace: "API key required"
```bash
# V√©rifier que HF_TOKEN est dans .env
cat .env | grep HF_TOKEN
```

### HuggingFace: "Gated model"
- Visitez la page du mod√®le sur HuggingFace
- Cliquez "Agree and access repository"
- Attendez l'approbation (g√©n√©ralement instantan√©)

## üí° Recommandations pour M1 16GB

**Pour la vitesse maximale:**
- Utilisez MLX avec quantization 4-bit
- Mod√®le recommand√©: `mlx-community/Qwen2.5-7B-Instruct-4bit`

**Pour la meilleure qualit√©:**
- Utilisez Ollama avec Llama 3.1 8B
- Mod√®le recommand√©: `llama3.1:8b`

**Pour le meilleur √©quilibre:**
- Utilisez Ollama avec Qwen 2.5 7B
- Mod√®le recommand√©: `qwen2.5:7b`

**Pour √©conomiser la RAM:**
- Utilisez des mod√®les 3B comme `llama3.2:3b`
- Ou utilisez MLX avec 4-bit quantization
